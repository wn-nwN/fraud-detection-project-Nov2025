{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nM_nYyEi8jp",
        "scrolled": true,
        "ExecuteTime": {
          "end_time": "2025-10-17T09:39:46.464444Z",
          "start_time": "2025-10-17T09:39:43.473767Z"
        }
      },
      "source": [
        "import pandas as pd\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "# df = pd.read_csv('drive/My Drive/Colab Notebooks/Base.csv')\n",
        "df = pd.read_csv('E:/python2222/data/Base.csv')\n",
        "\n",
        "# Specify the label column (the label column name in Base.csv is fraud_bool)\n",
        "label_col = \"fraud_bool\"\n",
        "if label_col not in df.columns:\n",
        "    raise KeyError(f\"Label column '{label_col}' not found. Please verify the label column name in the CSV and modify label_col accordingly.\")\n",
        "\n",
        "# Construct X, y (these variables will be used by the model later)\n",
        "y = df[label_col].astype(int)\n",
        "X = df.drop(columns=[label_col])\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(y.value_counts(dropna=False))\n",
        "print(\"Positive rate = {:.6f}\".format(y.mean()))\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())\n",
        "# For convenience in subsequent steps, save basic information\n",
        "print(\"\\nNote: We will use variables X, y, df as the main data objects in the following steps.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGYcpz-di8jt",
        "ExecuteTime": {
          "end_time": "2025-10-17T09:39:53.807425Z",
          "start_time": "2025-10-17T09:39:50.563568Z"
        }
      },
      "source": [
        "# === : Automatically identify numeric/categorical/ID/time/high-cardinality columns (check output after running) ===\n",
        "import pandas as pd\n",
        "# 1) Data types and unique value statistics for each column\n",
        "col_nunique = df.nunique(dropna=False).sort_values()\n",
        "col_dtype = df.dtypes\n",
        "summary_df = pd.DataFrame({\n",
        "    \"dtype\": col_dtype,\n",
        "    \"nunique\": col_nunique,\n",
        "    \"unique_ratio\": col_nunique / len(df),\n",
        "    \"missing\": df.isna().sum()\n",
        "}).sort_values(\"nunique\", ascending=False)\n",
        "display(summary_df.head(50))  # Display top 50 columns with more unique values\n",
        "# 2) Automatically separate numeric and categorical columns (for reference, manual verification needed)\n",
        "num_cols_auto = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
        "cat_cols_auto = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
        "print(\"\\nAutomatic detection (for reference only):\")\n",
        "print(\"Numeric columns (count={}, first 30 examples):\".format(len(num_cols_auto)), num_cols_auto[:30])\n",
        "print(\"Categorical columns (count={}, first 30 examples):\".format(len(cat_cols_auto)), cat_cols_auto[:30])\n",
        "# 3) Identify possible ID columns (high cardinality / unique_ratio close to 1 or very large nunique)\n",
        "possible_id_cols = []\n",
        "for c in df.columns:\n",
        "    if summary_df.loc[c, \"unique_ratio\"] > 0.95:\n",
        "        possible_id_cols.append((c, int(summary_df.loc[c, \"nunique\"])))\n",
        "# Also consider column names explicitly containing 'id' as possible IDs\n",
        "for c in df.columns:\n",
        "    if \"id\" in c.lower() or c.lower().endswith(\"_id\"):\n",
        "        if c not in [x[0] for x in possible_id_cols]:\n",
        "            possible_id_cols.append((c, int(summary_df.loc[c, \"nunique\"])))\n",
        "print(\"\\nSuggested ID-like columns to check (unique_ratio>0.95 or name contains 'id'):\")\n",
        "print(possible_id_cols[:50])\n",
        "# 4) Identify possible time columns (column names containing date/time or successfully parsed as datetimes)\n",
        "possible_time_cols = []\n",
        "date_like_names = [c for c in df.columns if any(k in c.lower() for k in (\"date\",\"time\",\"day\",\"month\",\"year\",\"ts\"))]\n",
        "for c in date_like_names:\n",
        "    # Try parsing the first 1000 non-null values and check the success rate\n",
        "    sample = df[c].dropna().astype(str).head(1000)\n",
        "    parsed = pd.to_datetime(sample, errors=\"coerce\")\n",
        "    success_ratio = parsed.notna().mean() if len(parsed)>0 else 0.0\n",
        "    if success_ratio > 0.8:\n",
        "        possible_time_cols.append((c, success_ratio))\n",
        "    else:\n",
        "        # Record but also note if success rate is low\n",
        "        possible_time_cols.append((c, success_ratio))\n",
        "print(\"\\ndate/time-like columns (with parsing success rate):\")\n",
        "print(possible_time_cols)\n",
        "# 5) High-cardinality categorical columns (categorical columns with large unique count, not friendly for OneHot)\n",
        "high_card_cat = []\n",
        "for c in cat_cols_auto:\n",
        "    nun = int(summary_df.loc[c,\"nunique\"])\n",
        "    if nun > 100:  # Threshold can be adjusted (100 is just an empirical value)\n",
        "        high_card_cat.append((c, nun))\n",
        "print(\"\\nHigh-cardinality categorical columns (nunique>100):\")\n",
        "print(high_card_cat[:50])\n",
        "# 6) Top few values for each categorical column (for confirmation)\n",
        "print(\"\\nExample values for categorical columns (showing up to 10 most frequent values per column):\")\n",
        "for c in cat_cols_auto[:50]:\n",
        "    top = df[c].value_counts(dropna=False).head(10)\n",
        "    print(f\"== {c} (nunique={int(summary_df.loc[c,'nunique'])}) ==\")\n",
        "    print(top.to_dict())\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBU3TL7Ni8ju",
        "ExecuteTime": {
          "end_time": "2025-10-17T09:39:59.418893Z",
          "start_time": "2025-10-17T09:39:59.413249Z"
        }
      },
      "source": [
        "# Columns to exclude from features\n",
        "exclude_cols = [ \"prev_address_months_count\", # Features with excessive missing samples that are difficult to impute\n",
        "                 \"month\"]  # Timestamp\n",
        "\n",
        "# === Organize num_cols/cat_cols, build ColumnTransformer, and perform stratified train/test split ===\n",
        "\n",
        "num_cols = ['income', 'name_email_similarity', 'current_address_months_count', 'customer_age',\n",
        "            'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h',\n",
        "            'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w',\n",
        "            'credit_risk_score', 'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes']\n",
        "\n",
        "cat_cols = ['payment_type', 'employment_status', 'email_is_free', 'housing_status', 'phone_home_valid',\n",
        "            'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'source', 'device_os',\n",
        "            'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count']\n",
        "\n",
        "print(\"Final num_cols count:\", len(num_cols))\n",
        "print(\"Final cat_cols count:\", len(cat_cols))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkp9ofk2SKnc",
        "ExecuteTime": {
          "end_time": "2025-10-17T09:40:14.436272Z",
          "start_time": "2025-10-17T09:40:05.454936Z"
        }
      },
      "source": [
        "# Train/test split (stratified split with stratify=y to prevent positive class distribution imbalance)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import sklearn\n",
        "\n",
        "X = df.drop(columns=[label_col])\n",
        "y = df[label_col].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Dataset split completed. Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "print(\"Train positives:\", int(y_train.sum()), \" Test positives:\", int(y_test.sum()))\n",
        "\n",
        "# Numeric features: median imputation + standardization\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical features: constant imputation + OneHot (automatically select parameters based on sklearn version)\n",
        "if sklearn.__version__ >= \"1.2\":\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
        "    ])\n",
        "else:  # Compatible with older versions\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
        "    ])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"num\", numeric_transformer, num_cols),\n",
        "    (\"cat\", categorical_transformer, cat_cols)\n",
        "], remainder=\"drop\", sparse_threshold=0.3)\n",
        "\n",
        "# Fit preprocessor (only fit on training set to avoid data leakage), and transform train/test sets for inspection\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "X_train_t = preprocessor.transform(X_train)\n",
        "X_test_t = preprocessor.transform(X_test)\n",
        "print(\"Transformed types:\", type(X_train_t), type(X_test_t))\n",
        "print(\"Transformed shapes (train/test):\", X_train_t.shape, X_test_t.shape)\n",
        "\n",
        "# Simply extract feature names after preprocessing (for plotting feature importance later)\n",
        "def get_feature_names_from_preprocessor(preproc):\n",
        "    feature_names = []\n",
        "    for name, trans, cols in preproc.transformers_:\n",
        "        if name == \"num\":\n",
        "            feature_names.extend(cols)\n",
        "        elif name == \"cat\":\n",
        "            ohe = trans.named_steps[\"onehot\"]\n",
        "            try:\n",
        "                ohe_names = ohe.get_feature_names_out(cols)\n",
        "            except Exception:\n",
        "                # Compatible with older sklearn versions\n",
        "                ohe_names = []\n",
        "                for i, col in enumerate(cols):\n",
        "                    cats = ohe.categories_[i]\n",
        "                    ohe_names.extend([f\"{col}__{str(val)}\" for val in cats])\n",
        "            feature_names.extend(list(ohe_names))\n",
        "    return feature_names\n",
        "\n",
        "feat_names = get_feature_names_from_preprocessor(preprocessor)\n",
        "print(\"Number of post-transform features (approx):\", len(feat_names))\n",
        "print(\"First 30 feature names:\", feat_names[:30])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T09:58:55.114948Z",
          "start_time": "2025-10-17T09:58:55.091659Z"
        },
        "id": "cAsaxLYsnarU"
      },
      "source": [
        "# === 2025.10.17\n",
        "import os, inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Compatibility wrapper\n",
        "def _get_feat_names_compat(preproc, X_columns):\n",
        "    # 1) If your function exists and has input_features parameter\n",
        "    try:\n",
        "        if 'get_feature_names_from_preprocessor' in globals():\n",
        "            sig = inspect.signature(get_feature_names_from_preprocessor)\n",
        "            if 'input_features' in sig.parameters:\n",
        "                return get_feature_names_from_preprocessor(preproc, input_features=X_columns)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) If your function exists but without input_features parameter\n",
        "    try:\n",
        "        if 'get_feature_names_from_preprocessor' in globals():\n",
        "            # Call your original version: def get_feature_names_from_preprocessor(preproc):\n",
        "            return get_feature_names_from_preprocessor(preproc)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 3) Fallback: manually construct feature names\n",
        "    feature_names = []\n",
        "    try:\n",
        "        for name, trans, cols in preproc.transformers_:\n",
        "            if name == \"num\":\n",
        "                # Add numeric column names directly\n",
        "                feature_names.extend(list(cols))\n",
        "            elif name == \"cat\":\n",
        "                # Expand categorical OneHot\n",
        "                ohe = trans.named_steps.get(\"onehot\", None)\n",
        "                if ohe is not None:\n",
        "                    try:\n",
        "                        ohe_names = ohe.get_feature_names_out(cols)\n",
        "                    except Exception:\n",
        "                        # Compatible with older sklearn versions\n",
        "                        ohe_names = []\n",
        "                        for i, col in enumerate(cols):\n",
        "                            cats = ohe.categories_[i]\n",
        "                            ohe_names.extend([f\"{col}__{str(val)}\" for val in cats])\n",
        "                    feature_names.extend(list(ohe_names))\n",
        "                else:\n",
        "                    # If no onehot step, append original columns directly\n",
        "                    feature_names.extend(list(cols))\n",
        "        return feature_names\n",
        "    except Exception as e:\n",
        "        print(\"Fallback building feature names failed:\", e)\n",
        "        # Last resort: fall back to original columns\n",
        "        return list(X_columns)\n",
        "\n",
        "def _get_preproc_and_names_from_pipeline(pipeline, X_columns):\n",
        "    if hasattr(pipeline, 'named_steps') and 'preproc' in pipeline.named_steps:\n",
        "        preproc = pipeline.named_steps['preproc']\n",
        "    else:\n",
        "        preproc = preprocessor   # Fallback\n",
        "    feat_names = _get_feat_names_compat(preproc, X_columns)\n",
        "    return preproc, feat_names\n",
        "\n",
        "# === Plot: Top20 horizontal bar ===\n",
        "def _plot_top_bar(importance_df, model_tag=\"Model\", top_n=20, figsize=(10,7)):\n",
        "    df = importance_df.head(top_n).sort_values('importance', ascending=True)\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    ax = sns.barplot(x='importance', y='feature', data=df, orient='h')\n",
        "    plt.title(f\"{model_tag} - Top {top_n} Feature Importance\")\n",
        "    plt.xlabel(\"Importance\"); plt.ylabel(\"Feature\")\n",
        "    for i,(v,f) in enumerate(zip(df['importance'], df['feature'])):\n",
        "        ax.text(v + 1e-6, i, f\"{v:.4f}\", va='center')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# === Plot: Cumulative curve ===\n",
        "def _plot_cumulative(importance_df, model_tag=\"Model\", top_n=50, figsize=(8,4)):\n",
        "    df = importance_df.copy().head(top_n).reset_index(drop=True)\n",
        "    total = importance_df['importance'].sum() or 1.0\n",
        "    cum_pct = df['importance'].cumsum()/total*100\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(range(1, len(cum_pct)+1), cum_pct, marker='o')\n",
        "    plt.xlabel(\"Number of features included\"); plt.ylabel(\"Cumulative importance (%)\")\n",
        "    plt.title(f\"{model_tag} cumulative importance (%)\")\n",
        "    plt.grid(True); plt.axhline(80, color='gray', linestyle='--', linewidth=1)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# === Plot: Permutation uncertainty (violin & box) ===\n",
        "def _plot_perm_uncertainty(perm_dict, model_tag=\"Model\", top_n=20, figsize=(10,6)):\n",
        "    if perm_dict is None: return\n",
        "    all_imps = perm_dict['all']; means = perm_dict['means']; feats = list(perm_dict['feature_names'])\n",
        "    order = np.argsort(means)[::-1][:top_n]\n",
        "    sel = [feats[i] for i in order]\n",
        "    rows=[]\n",
        "    for i in order:\n",
        "        for v in all_imps[i]:\n",
        "            rows.append({'feature': feats[i], 'importance': v})\n",
        "    long_df = pd.DataFrame(rows)\n",
        "    plt.figure(figsize=figsize); sns.violinplot(x='importance', y='feature', data=long_df, order=sel, scale='width')\n",
        "    plt.title(f\"{model_tag} - Permutation distributions (violin)\"); plt.tight_layout(); plt.show()\n",
        "    plt.figure(figsize=figsize); sns.boxplot(x='importance', y='feature', data=long_df, order=sel)\n",
        "    plt.title(f\"{model_tag} - Permutation distributions (box)\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "# === Core: Unified computation + plotting + export ===\n",
        "def run_importance_all_models(pipeline, X, y, model_tag=\"Model\",\n",
        "                              top_n=20,\n",
        "                              prefer='auto',         # 'auto'|'always_perm'|'no_perm'\n",
        "                              n_repeats=10,\n",
        "                              max_subsample=3000,    # Maximum subsample size for permutation\n",
        "                              max_samples_per_repeat=2000,\n",
        "                              n_jobs=1,              # Recommend 1 for Windows\n",
        "                              scoring='f1_weighted',\n",
        "                              out_dir=\"./importance_outputs\"):\n",
        "    \"\"\"\n",
        "    prefer:\n",
        "      - 'auto'       : Prefer coef_/feature_importances_, otherwise use permutation\n",
        "      - 'always_perm': Force permutation (for SVM rbf, etc.)\n",
        "      - 'no_perm'    : Only use direct importance; skip if not available\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Get estimator & preproc & expanded names\n",
        "    if hasattr(pipeline, 'named_steps'):\n",
        "        est = pipeline.named_steps[list(pipeline.named_steps.keys())[-1]]\n",
        "    else:\n",
        "        est = pipeline\n",
        "    preproc, feat_names = _get_preproc_and_names_from_pipeline(pipeline, X.columns)\n",
        "\n",
        "    # ========== Case A: Has direct importance ==========\n",
        "    if prefer in ('auto','no_perm') and hasattr(est, 'feature_importances_'):\n",
        "        imps = est.feature_importances_\n",
        "        imp_df = pd.DataFrame({'feature': feat_names, 'importance': imps}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "        imp_df.to_csv(os.path.join(out_dir, f\"{model_tag}_feature_importances.csv\"), index=False, encoding='utf-8-sig')\n",
        "        _plot_top_bar(imp_df, model_tag, top_n); _plot_cumulative(imp_df, model_tag, max(50, top_n))\n",
        "        return imp_df, None\n",
        "\n",
        "    if prefer in ('auto','no_perm') and hasattr(est, 'coef_'):\n",
        "        coefs = est.coef_\n",
        "        imps = np.mean(np.abs(coefs), axis=0) if coefs.ndim>1 else np.abs(coefs).ravel()\n",
        "        imp_df = pd.DataFrame({'feature': feat_names, 'importance': imps}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "        imp_df.to_csv(os.path.join(out_dir, f\"{model_tag}_coef_importances.csv\"), index=False, encoding='utf-8-sig')\n",
        "        _plot_top_bar(imp_df, model_tag, top_n); _plot_cumulative(imp_df, model_tag, max(50, top_n))\n",
        "        return imp_df, None\n",
        "\n",
        "    if prefer == 'no_perm':\n",
        "        print(f\"[{model_tag}] no direct importances; permutation disabled -> skipped.\")\n",
        "        return None, None\n",
        "\n",
        "    # ========== Case B: Do permutation (transform to expanded X, then permute on subsample) ==========\n",
        "    # Transform once\n",
        "    X_trans_full = preproc.transform(X)\n",
        "    n_full = X_trans_full.shape[0]\n",
        "    # Subsample (stratified, for speed)\n",
        "    n_take = min(max_subsample, n_full)\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=n_take, random_state=42)\n",
        "    idx = next(sss.split(np.zeros(n_full), y))[0]\n",
        "    X_trans = X_trans_full[idx] if hasattr(X_trans_full, \"tocsr\") else X_trans_full[idx,:]\n",
        "    y_sub = np.asarray(y)[idx]\n",
        "\n",
        "    # Align y dtype\n",
        "    y_arr = y_sub\n",
        "    if hasattr(est, 'classes_'):\n",
        "        cls = est.classes_\n",
        "        try:\n",
        "            y_arr = y_arr.astype(getattr(cls, 'dtype', y_arr.dtype), copy=False)\n",
        "        except Exception:\n",
        "            y_arr = y_arr.astype(str)\n",
        "\n",
        "    # scorer & permutation\n",
        "    scorer = make_scorer(f1_score, average=scoring.split('_')[-1]) if scoring!='accuracy' else 'accuracy'\n",
        "    try:\n",
        "        res = permutation_importance(est, X_trans, y_arr, n_repeats=n_repeats,\n",
        "                                     random_state=42, n_jobs=n_jobs, scoring=scorer,\n",
        "                                     max_samples=min(max_samples_per_repeat, X_trans.shape[0]))\n",
        "    except TypeError:\n",
        "        res = permutation_importance(est, X_trans, y_arr, n_repeats=n_repeats,\n",
        "                                     random_state=42, n_jobs=n_jobs, scoring=scorer)\n",
        "\n",
        "    means = res.importances_mean\n",
        "    all_imps = res.importances\n",
        "    imp_df = pd.DataFrame({'feature': feat_names, 'importance': means}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "    imp_df.to_csv(os.path.join(out_dir, f\"{model_tag}_perm_importances_mean.csv\"), index=False, encoding='utf-8-sig')\n",
        "    np.save(os.path.join(out_dir, f\"{model_tag}_perm_importances_all.npy\"), all_imps)\n",
        "\n",
        "    # Plot\n",
        "    _plot_top_bar(imp_df, model_tag, top_n)\n",
        "    _plot_cumulative(imp_df, model_tag, max(50, top_n))\n",
        "    _plot_perm_uncertainty({'means':means,'all':all_imps,'feature_names':feat_names}, model_tag, top_n)\n",
        "    return imp_df, {'means':means,'all':all_imps,'feature_names':feat_names}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NXL0CRS1W2Z"
      },
      "source": [
        "## 3.1 Classical ML Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc2JWqJKBRwH"
      },
      "source": [
        "### 3.1.0 Pre-processing for classical ML Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7Y2b0vD8mU7"
      },
      "source": [
        "### 3.1.1 SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMH59hIo5ykh",
        "ExecuteTime": {
          "end_time": "2025-10-17T09:40:50.030414Z",
          "start_time": "2025-10-17T09:40:27.711343Z"
        }
      },
      "source": [
        "# === SVM hyperparameter tuning (Step 1: GridSearch on small subset, safe and efficient) ===\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ----- Configuration (if machine resources are limited, reduce dev_sample to 20000 or 10000) -----\n",
        "dev_sample = 10000  # Number of samples for GridSearch (stratified sampling)\n",
        "cv_splits = 3       # Number of CV folds (3 is faster, 5 is more stable)\n",
        "random_state = 42\n",
        "\n",
        "# ----- Stratified sampling from training set to create dev set and dev_validation -----\n",
        "if len(X_train) > dev_sample:\n",
        "    X_dev, _, y_dev, _ = train_test_split(X_train, y_train, train_size=dev_sample, stratify=y_train, random_state=random_state)\n",
        "else:\n",
        "    X_dev = X_train.copy()\n",
        "    y_dev = y_train.copy()\n",
        "\n",
        "print(\"Using dev sample size:\", len(X_dev), \" Positive examples in dev:\", int(y_dev.sum()))\n",
        "\n",
        "# ----- Further split dev into train/val (we also specify StratifiedKFold for GridSearch's internal CV) -----\n",
        "# Here we directly pass cv=StratifiedKFold(...) to GridSearchCV\n",
        "cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "# ----- Build pipeline (preprocessor already constructed in previous step: preprocessor) -----\n",
        "pipeline_svc = Pipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('svc', SVC(random_state=random_state))  # Don't set probability=True (saves time), use decision_function for AUC\n",
        "])\n",
        "\n",
        "# ----- Parameter grid (keeping your original parameter options) -----\n",
        "param_grid = {\n",
        "    'svc__C': [0.1, 1, 10],\n",
        "    'svc__kernel': ['linear', 'rbf'],\n",
        "    'svc__class_weight': ['balanced', None],\n",
        "    'svc__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# ----- GridSearchCV (run in parallel) -----\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipeline_svc,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_weighted',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ----- Time and fit (note: may still take several minutes or longer, depending on dev_sample and kernel) -----\n",
        "start_time = time.time()\n",
        "grid.fit(X_dev, y_dev)\n",
        "end_time = time.time()\n",
        "print(f\"GridSearch done in {end_time - start_time:.1f} seconds\")\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV score (f1_weighted):\", grid.best_score_)\n",
        "\n",
        "# ----- Get the best estimator (including preprocessor) -----\n",
        "best_svm_pipeline = grid.best_estimator_\n",
        "\n",
        "# ----- Save best model for later loading (overwrite old svm_model_binary.pkl) -----\n",
        "import joblib\n",
        "joblib.dump(best_svm_pipeline, \"svm_model_binary.pkl\")\n",
        "print(\"Saved best SVM pipeline to svm_model_binary.pkl\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T10:00:58.076598Z",
          "start_time": "2025-10-17T09:59:31.638035Z"
        },
        "id": "yR6hPnzknarZ"
      },
      "source": [
        "# 2025.10.17: SVM - importance & plots (force permutation for rbf/nonlinear kernels)\n",
        "svm_imp_df, svm_perm = run_importance_all_models(\n",
        "    best_svm_pipeline, X_train, y_train,\n",
        "    model_tag=\"SVM\",\n",
        "    top_n=20,\n",
        "    prefer='always_perm',   # Force permutation (required for SVM rbf)\n",
        "    n_repeats=10,           # Start with 10; can increase to 20 for final publication\n",
        "    max_subsample=3000,     # Subsample for speed; can use 5000~10000 on better machines\n",
        "    n_jobs=1                # Single thread recommended for Windows stability\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T06:32:36.915881Z",
          "start_time": "2025-10-17T06:32:36.907452Z"
        },
        "id": "af4saf4evIua"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "def bootstrap_f1_ci(y_true, y_pred, n_iterations=1000, average='weighted'):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    f1_scores = []\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        indices = resample(np.arange(len(y_true)))\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "        f1 = f1_score(y_true[indices], y_pred[indices], average=average)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    f1_mean = np.mean(f1_scores)\n",
        "    ci_lower = np.percentile(f1_scores, 2.5)\n",
        "    ci_upper = np.percentile(f1_scores, 97.5)\n",
        "    return f1_mean, ci_lower, ci_upper\n",
        "\n",
        "\n",
        "def bootstrap_auc_ci(y_true, y_scores, n_iterations=1000):\n",
        "    y_true = np.array(y_true)\n",
        "    y_scores = np.array(y_scores)\n",
        "    auc_scores = []\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        indices = resample(np.arange(len(y_true)))\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "        fpr, tpr, _ = roc_curve(y_true[indices], y_scores[indices])\n",
        "        auc_score = auc(fpr, tpr)\n",
        "        auc_scores.append(auc_score)\n",
        "\n",
        "    ci_lower = np.percentile(auc_scores, 2.5)\n",
        "    ci_upper = np.percentile(auc_scores, 97.5)\n",
        "    return np.mean(auc_scores), ci_lower, ci_upper\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels=['0', '1']):\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_roc_curve(y_true, y_scores, label_prefix=\"Model\"):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f'{label_prefix} AUC = {roc_auc:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {label_prefix}')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name=\"Model\", use_proba=False):\n",
        "    print(f\"\\n--- Evaluation Report: {model_name} ---\\n\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    f1_mean, f1_ci_low, f1_ci_high = bootstrap_f1_ci(y_test, y_pred)\n",
        "    print(f\"Weighted F1 Score: {f1_mean:.4f}\")\n",
        "    print(f\"95% CI for F1 Score: [{f1_ci_low:.4f}, {f1_ci_high:.4f}]\")\n",
        "\n",
        "    plot_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # ROC + AUC\n",
        "    if use_proba:\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_scores = model.decision_function(X_test)\n",
        "\n",
        "    auc_score, auc_ci_low, auc_ci_high = bootstrap_auc_ci(y_test, y_scores)\n",
        "    print(f\"AUC Score: {auc_score:.4f}\")\n",
        "    print(f\"95% CI for AUC: [{auc_ci_low:.4f}, {auc_ci_high:.4f}]\")\n",
        "\n",
        "    plot_roc_curve(y_test, y_scores, label_prefix=model_name)\n",
        "\n",
        "\n",
        "# --- Usage Example ---\n",
        "# For SVM with decision_function:\n",
        "# evaluate_model(SVM, X_test_1, y_test_1, model_name=\"SVM\", use_proba=False)\n",
        "\n",
        "# For models like RandomForest or LogisticRegression with predict_proba:\n",
        "# evaluate_model(rf_model, X_test, y_test, model_name=\"Random Forest\", use_proba=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T06:34:00.266061Z",
          "start_time": "2025-10-17T06:32:46.429453Z"
        },
        "id": "eNd7biGCGAb6"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Load model\n",
        "loaded_svm = joblib.load(\"svm_model_binary.pkl\")\n",
        "\n",
        "# Call evaluation function\n",
        "evaluate_model(loaded_svm, X_test, y_test, model_name=\"SVM\", use_proba=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PewSwsEtYJmn"
      },
      "source": [
        "### 3.1.2 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7RlijD5267",
        "ExecuteTime": {
          "end_time": "2025-10-17T10:05:48.535071Z",
          "start_time": "2025-10-17T10:05:27.575913Z"
        }
      },
      "source": [
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# ----- Build pipeline -----\n",
        "pipeline_lr = Pipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('lr', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# ----- Parameter grid -----\n",
        "param_grid_lr = {\n",
        "    'lr__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'lr__solver': ['liblinear', 'lbfgs'],\n",
        "    'lr__penalty': ['l2'],\n",
        "    'lr__class_weight': ['balanced', None]\n",
        "}\n",
        "\n",
        "# ----- GridSearchCV -----\n",
        "grid_lr = GridSearchCV(\n",
        "    estimator=pipeline_lr,\n",
        "    param_grid=param_grid_lr,\n",
        "    scoring='f1_weighted',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "grid_lr.fit(X_dev, y_dev)\n",
        "end_time = time.time()\n",
        "print(f\"Grid search (LogReg) completed in {end_time - start_time:.1f} seconds\")\n",
        "\n",
        "print(\"Best parameters (LogReg):\", grid_lr.best_params_)\n",
        "print(\"Best CV score (f1_weighted):\", grid_lr.best_score_)\n",
        "\n",
        "# Save best model\n",
        "best_lr_pipeline = grid_lr.best_estimator_\n",
        "joblib.dump(best_lr_pipeline, \"lr_model_binary.pkl\")\n",
        "print(\"Best Logistic Regression pipeline saved to lr_model_binary.pkl\")\n",
        "\n",
        "\n",
        "# Force use of parameters we know are correct\n",
        "pipeline_lr_fixed = Pipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('lr', LogisticRegression(\n",
        "        C=1,  # Use best C value found from grid search\n",
        "        class_weight='balanced',  # Force balanced weights\n",
        "        penalty='l2',\n",
        "        solver='liblinear',\n",
        "        random_state=42,\n",
        "        max_iter=1000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train on full training set\n",
        "pipeline_lr_fixed.fit(X_train, y_train)\n",
        "\n",
        "# Simple evaluation\n",
        "y_pred = pipeline_lr_fixed.predict(X_test)\n",
        "print(\"--- LR-Fixed Results ---\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T10:06:38.659885Z",
          "start_time": "2025-10-17T10:06:38.132768Z"
        },
        "id": "N07WZHLWnarb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "lr_imp_df, lr_perm = run_importance_all_models(\n",
        "    best_lr_pipeline, X_train, y_train,\n",
        "    model_tag=\"LogisticRegression\",\n",
        "    top_n=20,\n",
        "    prefer='auto'    # Will automatically use coef_\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T06:35:57.352389Z",
          "start_time": "2025-10-17T06:35:04.550280Z"
        },
        "id": "DBUEx8vZxNHg"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "evaluate_model(best_lr_pipeline, X_test, y_test, model_name=\"Logistic Regression\", use_proba=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOFKjTzEZgwT"
      },
      "source": [
        "### 3.1.3 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuDypQKN6IU3",
        "ExecuteTime": {
          "end_time": "2025-10-17T10:09:53.924294Z",
          "start_time": "2025-10-17T10:09:07.292122Z"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# ----- Build pipeline -----\n",
        "pipeline_rf = Pipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# ----- Parameter grid -----\n",
        "param_grid_rf = {\n",
        "    'rf__n_estimators': [50, 100, 200],       # Number of trees\n",
        "    'rf__max_depth': [10, 20, None],          # Maximum depth of trees\n",
        "    'rf__min_samples_split': [2, 5, 10],      # Minimum samples to split a node\n",
        "    'rf__min_samples_leaf': [1, 2, 4],        # Minimum samples at a leaf node\n",
        "    'rf__class_weight': [None, 'balanced']    # Handle class imbalance\n",
        "}\n",
        "\n",
        "# ----- GridSearchCV -----\n",
        "grid_rf = GridSearchCV(\n",
        "    estimator=pipeline_rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    scoring='f1_weighted',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "grid_rf.fit(X_dev, y_dev)\n",
        "end_time = time.time()\n",
        "print(f\"Grid search (RandomForest) completed in {end_time - start_time:.1f} seconds\")\n",
        "\n",
        "print(\"Best parameters (RandomForest):\", grid_rf.best_params_)\n",
        "print(\"Best CV score (f1_weighted):\", grid_rf.best_score_)\n",
        "\n",
        "# Save best model\n",
        "best_rf_pipeline = grid_rf.best_estimator_\n",
        "joblib.dump(best_rf_pipeline, \"rf_model_binary.pkl\")\n",
        "print(\"Best RandomForest pipeline saved to rf_model_binary.pkl\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T10:11:15.503388Z",
          "start_time": "2025-10-17T10:11:14.900825Z"
        },
        "id": "I-B5I_bjnarc"
      },
      "cell_type": "code",
      "source": [
        "# 2025.10.17\n",
        "rf_imp_df, rf_perm = run_importance_all_models(\n",
        "    best_rf_pipeline, X_train, y_train,\n",
        "    model_tag=\"RandomForest\",\n",
        "    top_n=20,\n",
        "    prefer='auto'    # Will automatically use feature_importances\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T06:38:11.619015Z",
          "start_time": "2025-10-17T06:36:53.883537Z"
        },
        "id": "bC7zxS8ux-8P"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test dataset\n",
        "evaluate_model(best_rf_pipeline, X_test, y_test, model_name=\"Random Forest\", use_proba=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxf9Kwwjgt1Q"
      },
      "source": [
        "### 3.1.4 LGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "motY8sH96gRm",
        "ExecuteTime": {
          "end_time": "2025-10-17T10:12:19.108684Z",
          "start_time": "2025-10-17T10:11:48.542735Z"
        }
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# ----- Build pipeline -----\n",
        "pipeline_lgbm = Pipeline([\n",
        "    ('preproc', preprocessor),\n",
        "    ('lgbm', LGBMClassifier(random_state=42, n_jobs=-1, importance_type=\"gain\"))\n",
        "])\n",
        "\n",
        "# ----- Parameter grid -----\n",
        "param_grid_lgbm = {\n",
        "    'lgbm__n_estimators': [50, 100],\n",
        "    'lgbm__learning_rate': [0.01, 0.1],\n",
        "    'lgbm__max_depth': [-1, 10],\n",
        "    'lgbm__num_leaves': [31, 50],\n",
        "    'lgbm__min_child_samples': [10, 20],\n",
        "    'lgbm__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# ----- GridSearchCV -----\n",
        "grid_lgbm = GridSearchCV(\n",
        "    estimator=pipeline_lgbm,\n",
        "    param_grid=param_grid_lgbm,\n",
        "    scoring='f1_weighted',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ----- Start grid search -----\n",
        "start_time = time.time()\n",
        "grid_lgbm.fit(X_dev, y_dev)\n",
        "end_time = time.time()\n",
        "print(f\"Grid search (LightGBM) completed in {end_time - start_time:.1f} seconds\")\n",
        "\n",
        "print(\"Best parameters (LightGBM):\", grid_lgbm.best_params_)\n",
        "print(\"Best CV score (f1_weighted):\", grid_lgbm.best_score_)\n",
        "\n",
        "# ----- Save best model -----\n",
        "best_lgbm_pipeline = grid_lgbm.best_estimator_\n",
        "joblib.dump(best_lgbm_pipeline, \"lgbm_model_binary.pkl\")\n",
        "print(\"Best LightGBM pipeline saved to lgbm_model_binary.pkl\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T10:13:30.269149Z",
          "start_time": "2025-10-17T10:13:29.725953Z"
        },
        "id": "VeAcGLGTnarc"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "lgbm_imp_df, lgbm_perm = run_importance_all_models(\n",
        "    best_lgbm_pipeline, X_train, y_train,\n",
        "    model_tag=\"LightGBM\",\n",
        "    top_n=20,\n",
        "    prefer='auto'    # Will automatically use feature_importances\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T06:50:07.472336Z",
          "start_time": "2025-10-17T06:48:48.110493Z"
        },
        "id": "6bSO_00Zyj3R"
      },
      "outputs": [],
      "source": [
        "# evaluate\n",
        "evaluate_model(best_lgbm_pipeline, X_test, y_test, model_name=\"LightGBM\", use_proba=True)\n"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T10:16:13.785419Z",
          "start_time": "2025-10-17T10:16:13.319768Z"
        },
        "id": "l2MYiPVLnarc"
      },
      "cell_type": "code",
      "source": [
        "# Multi-model comparison heatmap\n",
        "def build_compare_matrix(model_imp_map, top_k=50, fillna=0.0):\n",
        "    feats = set()\n",
        "    for m, df in model_imp_map.items():\n",
        "        feats |= set(df.head(top_k)['feature'].tolist())\n",
        "    feats = sorted(list(feats))\n",
        "    mat = pd.DataFrame(index=feats)\n",
        "    for m, df in model_imp_map.items():\n",
        "        s = df.set_index('feature')['importance']\n",
        "        mat[m] = s.reindex(feats).fillna(fillna)\n",
        "    return mat\n",
        "\n",
        "def plot_compare_heatmap(mat_df):\n",
        "    h = max(4, mat_df.shape[0] * 0.18)\n",
        "    plt.figure(figsize=(10, h))\n",
        "    sns.heatmap(mat_df, cmap='viridis', linewidths=0.5)\n",
        "    plt.title(\"Model comparison: feature importances\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "model_imp_map = {}\n",
        "for name, df in [('LogReg', lr_imp_df), ('RandomForest', rf_imp_df), ('LightGBM', lgbm_imp_df), ('SVM', svm_imp_df)]:\n",
        "    if df is not None:\n",
        "        model_imp_map[name] = df\n",
        "\n",
        "if len(model_imp_map) >= 2:\n",
        "    mat = build_compare_matrix(model_imp_map, top_k=50)\n",
        "    plot_compare_heatmap(mat)\n",
        "else:\n",
        "    print(\"Not enough models with importances to build heatmap.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEX8CIK8TUe6"
      },
      "source": [
        "## 3.2  DL Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqpnRHGhjAoN"
      },
      "source": [
        "### 3.2.2 GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haolyiMKKYjB",
        "ExecuteTime": {
          "end_time": "2025-10-17T10:14:58.615811Z",
          "start_time": "2025-10-17T10:14:47.078144Z"
        }
      },
      "source": [
        "# === (1) Use fitted parts of preprocessor to build DataLoaders for GRU (direct tabular input) ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "BATCH_SIZE_GRU = 256\n",
        "random_state = 42\n",
        "\n",
        "# Confirm preprocessor has been fitted\n",
        "if not hasattr(preprocessor, \"transformers_\"):\n",
        "    raise RuntimeError(\"preprocessor not fitted â€” please run preprocessor.fit(X_train) first (already done in LGBM code section)\")\n",
        "\n",
        "# 1) Get numeric pipeline (imputer+scaler) and reuse its transform\n",
        "#    If you want to directly reuse preprocessor.transform(X)[...], but we only need the numeric part's transformation\n",
        "num_pipeline = preprocessor.named_transformers_.get('num', None)\n",
        "if num_pipeline is None:\n",
        "    raise RuntimeError(\"No 'num' transformer in preprocessor, please check if num_cols is correct\")\n",
        "\n",
        "# 2) Get categories_ from categorical onehot encoder to generate label->index mapping\n",
        "cat_transformer = preprocessor.named_transformers_.get('cat', None)\n",
        "if cat_transformer is None:\n",
        "    raise RuntimeError(\"No 'cat' transformer in preprocessor, please check if cat_cols is correct\")\n",
        "\n",
        "# onehot step:\n",
        "if hasattr(cat_transformer, \"named_steps\") and 'onehot' in cat_transformer.named_steps:\n",
        "    ohe = cat_transformer.named_steps['onehot']\n",
        "else:\n",
        "    raise RuntimeError(\"onehot step not included in categorical transformer or differently named, please check preprocessor construction\")\n",
        "\n",
        "# categories_ is list of arrays, corresponding to the order of cat_cols\n",
        "ohe_categories = list(ohe.categories_)  # Each element is all categories for that column (from training set)\n",
        "# Build cat -> index map for each categorical column, keep unseen mapping to len(categories)\n",
        "cat_to_index = {}\n",
        "cat_num_embeddings = {}\n",
        "for i,c in enumerate(cat_cols):\n",
        "    cats = list(ohe_categories[i])\n",
        "    mapping = {str(v): idx for idx, v in enumerate(cats)}  # cat value -> 0..K-1\n",
        "    cat_to_index[c] = mapping\n",
        "    cat_num_embeddings[c] = len(cats) + 1  # +1 reserved for unseen\n",
        "\n",
        "# 3) Dataset class returning input_ids=(num_tensor,cat_tensor), label\n",
        "class TabularGRUDataset(Dataset):\n",
        "    def __init__(self, X_df, y_ser, num_cols, cat_cols, num_pipeline, cat_to_index):\n",
        "        self.X = X_df.reset_index(drop=True).copy()\n",
        "        self.y = y_ser.reset_index(drop=True).astype(int).copy()\n",
        "        self.num_cols = num_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.num_pipeline = num_pipeline\n",
        "        self.cat_to_index = cat_to_index\n",
        "\n",
        "        # numeric transformation using fitted numeric pipeline (imputer+scaler)\n",
        "        if len(self.num_cols) > 0:\n",
        "            # num_pipeline.transform accepts DataFrame or array\n",
        "            self.num_mat = self.num_pipeline.transform(self.X[self.num_cols])\n",
        "            # ensure float32\n",
        "            self.num_mat = np.array(self.num_mat, dtype=np.float32)\n",
        "        else:\n",
        "            self.num_mat = np.zeros((len(self.X), 0), dtype=np.float32)\n",
        "\n",
        "        # categorical -> integer indices using cat_to_index mapping\n",
        "        cat_mats = []\n",
        "        for c in self.cat_cols:\n",
        "            vals = self.X[c].fillna(\"__MISSING__\").astype(str).values\n",
        "            map_c = self.cat_to_index[c]\n",
        "            int_vals = []\n",
        "            unseen_idx = len(map_c)   # reserved index for unseen\n",
        "            for v in vals:\n",
        "                int_vals.append(map_c.get(v, unseen_idx))\n",
        "            cat_mats.append(np.array(int_vals, dtype=np.int64))\n",
        "        if len(cat_mats) > 0:\n",
        "            self.cat_mat = np.stack(cat_mats, axis=1)\n",
        "        else:\n",
        "            self.cat_mat = np.zeros((len(self.X), 0), dtype=np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        num_row = torch.from_numpy(self.num_mat[idx])    # (n_num,)\n",
        "        cat_row = torch.from_numpy(self.cat_mat[idx])    # (n_cat,)\n",
        "        label = torch.tensor(int(self.y.iloc[idx]), dtype=torch.long)\n",
        "        return {'input_ids': (num_row, cat_row), 'label': label}\n",
        "\n",
        "# 4) Create datasets/loaders: if X_val not present, split X_train into train+val (consistent with LGBM)\n",
        "# Make sure X_train, X_test, y_train, y_test exist in workspace\n",
        "if 'X_val' not in globals():\n",
        "    # merge labels into X_train copy for splitting convenience\n",
        "    X_train_for_split = X_train.copy()\n",
        "    X_train_for_split[label_col] = y_train.values\n",
        "    X_train_df, X_val_df = train_test_split(X_train_for_split, test_size=0.2, stratify=X_train_for_split[label_col], random_state=random_state)\n",
        "    # separate y\n",
        "    y_train_df = X_train_df[label_col]\n",
        "    X_train_df = X_train_df.drop(columns=[label_col])\n",
        "    y_val_df = X_val_df[label_col]\n",
        "    X_val_df = X_val_df.drop(columns=[label_col])\n",
        "else:\n",
        "    X_train_df = X_train.copy()\n",
        "    y_train_df = y_train.copy()\n",
        "    X_val_df = X_val.copy()\n",
        "    y_val_df = y_val.copy()\n",
        "\n",
        "# ensure X_test has label\n",
        "X_test_df = X_test.copy()\n",
        "X_test_df[label_col] = y_test.values\n",
        "\n",
        "# Build dataset objects\n",
        "train_dataset_3 = TabularGRUDataset(X_train_df, y_train_df, num_cols, cat_cols, num_pipeline, cat_to_index)\n",
        "val_dataset_3   = TabularGRUDataset(X_val_df, y_val_df, num_cols, cat_cols, num_pipeline, cat_to_index)\n",
        "test_dataset_3  = TabularGRUDataset(X_test_df.drop(columns=[label_col]), X_test_df[label_col], num_cols, cat_cols, num_pipeline, cat_to_index)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader_3 = DataLoader(train_dataset_3, batch_size=BATCH_SIZE_GRU, shuffle=True)\n",
        "val_loader_3   = DataLoader(val_dataset_3, batch_size=BATCH_SIZE_GRU, shuffle=False)\n",
        "test_loader_3  = DataLoader(test_dataset_3, batch_size=BATCH_SIZE_GRU, shuffle=False)\n",
        "\n",
        "# expose cat_num_embeddings dict for model construction\n",
        "# cat_num_embeddings already prepared above\n",
        "print(\"Built GRU loaders. Sizes:\", len(train_dataset_3), len(val_dataset_3), len(test_dataset_3))\n",
        "print(\"Categorical embedding sizes (per column):\")\n",
        "print({k: cat_num_embeddings[k] for k in list(cat_num_embeddings)[:10]})\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8eQecQ9M8YX",
        "ExecuteTime": {
          "end_time": "2025-10-17T10:17:36.489701Z",
          "start_time": "2025-10-17T10:17:36.480596Z"
        }
      },
      "source": [
        "\n",
        "\n",
        "# Define GRU Model\n",
        "import torch.nn as nn\n",
        "class GRUSentimentModel(nn.Module):\n",
        "    def __init__(self, num_cols, cat_cols, cat_num_embeddings, embedding_dim=128, hidden_dim=256, output_dim=2, dropout=0.3):\n",
        "        super(GRUSentimentModel, self).__init__()\n",
        "        self.num_cols = num_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.n_num = len(num_cols)\n",
        "        self.n_cat = len(cat_cols)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # numeric projection: shared linear (scalar -> embedding_dim)\n",
        "        self.num_proj = nn.Linear(1, embedding_dim)\n",
        "        # categorical embeddings: one embedding per categorical feature\n",
        "        self.cat_embeddings = nn.ModuleDict({\n",
        "            c: nn.Embedding(cat_num_embeddings[c], embedding_dim) for c in cat_cols\n",
        "        })\n",
        "        # GRU and classifier head\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: tuple (num_tensor, cat_tensor)\n",
        "        num_tensor, cat_tensor = input_ids\n",
        "        B = num_tensor.size(0)\n",
        "        emb_list = []\n",
        "        # numeric projection -> produce (B, n_num, embedding_dim)\n",
        "        if self.n_num > 0:\n",
        "            num_flat = num_tensor.view(-1,1)                    # (B*n_num,1)\n",
        "            num_proj_flat = self.num_proj(num_flat)             # (B*n_num, emb_dim)\n",
        "            num_proj = num_proj_flat.view(B, self.n_num, -1)    # (B, n_num, emb_dim)\n",
        "            emb_list.append(num_proj)\n",
        "        # categorical embeddings -> (B, n_cat, emb_dim)\n",
        "        if self.n_cat > 0:\n",
        "            cat_embs = []\n",
        "            for i, c in enumerate(self.cat_cols):\n",
        "                emb_i = self.cat_embeddings[c](cat_tensor[:, i])   # (B, emb_dim)\n",
        "                cat_embs.append(emb_i.unsqueeze(1))               # (B,1,emb_dim)\n",
        "            cat_embs = torch.cat(cat_embs, dim=1)\n",
        "            emb_list.append(cat_embs)\n",
        "        # concat into sequence (B, seq_len, emb_dim)\n",
        "        seq = torch.cat(emb_list, dim=1)\n",
        "        # GRU forward -> use last hidden state\n",
        "        _, hidden = self.gru(seq)\n",
        "        out = self.fc(self.dropout(hidden[-1]))\n",
        "        return out"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJgiSDTBNK1G",
        "ExecuteTime": {
          "end_time": "2025-10-17T11:26:31.717817Z",
          "start_time": "2025-10-17T10:17:53.439837Z"
        }
      },
      "source": [
        "\n",
        "\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Select device: use GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Optional: if using GPU, set random seed for reproducibility (optional)\n",
        "import random, numpy as np\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(1234)\n",
        "    # Display GPU name (for confirmation)\n",
        "    try:\n",
        "        print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def train_and_evaluate(hyperparams, train_loader, val_loader, vocab_size, output_dim, device):\n",
        "    embedding_dim = int(hyperparams['embedding_dim'])\n",
        "    hidden_dim = int(hyperparams['hidden_dim'])\n",
        "    lr = float(hyperparams['lr'])\n",
        "    epochs = int(hyperparams.get('epochs', 5))\n",
        "\n",
        "    # Initialize the model, loss, and optimizer\n",
        "    model = GRUSentimentModel(\n",
        "        num_cols=num_cols,\n",
        "        cat_cols=cat_cols,\n",
        "        cat_num_embeddings=cat_num_embeddings,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        output_dim=output_dim\n",
        "    ).to(device)\n",
        "\n",
        "    # **MODIFIED**: Compute class weights for the training set\n",
        "    all_train_labels = []\n",
        "    for batch in train_loader:\n",
        "        lbls = batch['label']\n",
        "        if isinstance(lbls, torch.Tensor):\n",
        "            lbls = lbls.cpu().numpy()\n",
        "        else:\n",
        "            lbls = np.array(lbls)\n",
        "        all_train_labels.extend(lbls.tolist())\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(all_train_labels),\n",
        "        y=all_train_labels\n",
        "    )\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)  # Convert to PyTorch tensor\n",
        "\n",
        "    # **MODIFIED**: Use CrossEntropyLoss with class weights\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(hyperparams['epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_loop = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\", leave=True, position=0)\n",
        "        for batch in train_loop:\n",
        "            num_tensor, cat_tensor = batch['input_ids']\n",
        "            num_tensor = num_tensor.to(device).float()\n",
        "            cat_tensor = cat_tensor.to(device).long()\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model((num_tensor, cat_tensor))\n",
        "            loss = criterion(outputs, labels)  # **MODIFIED**: Use CrossEntropyLoss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Validation evaluation\n",
        "    model.eval()\n",
        "    val_predictions, val_labels = [], []\n",
        "    val_loop = tqdm(val_loader, desc=\"Validating\", leave=True, position=0)\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loop:\n",
        "            num_tensor, cat_tensor = batch['input_ids']\n",
        "            num_tensor = num_tensor.to(device).float()\n",
        "            cat_tensor = cat_tensor.to(device).long()\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model((num_tensor, cat_tensor))\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_predictions.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate F1 score and accuracy\n",
        "    f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
        "    accuracy = accuracy_score(val_labels, val_predictions)\n",
        "    print(f\"Validation F-1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    return f1, accuracy, model\n",
        "\n",
        "\n",
        "def random_hyperparameter_tuning(train_loader, val_loader, vocab_size, output_dim, device, n_iter=20):\n",
        "    # Define hyperparameter ranges for random search\n",
        "    param_space = {\n",
        "        'embedding_dim': (150, 250),\n",
        "        'hidden_dim': (256, 768),\n",
        "        'lr': (np.log10(1e-4), np.log10(1e-3)),  # Log-uniform for learning rate\n",
        "        'epochs': (5, 10),  # **MODIFIED**: Removed `alpha` and `gamma` since FocalLoss is no longer used\n",
        "    }\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Random search iterations\n",
        "    with tqdm(total=n_iter, desc=\"Random Search Tuning\", leave=True, position=0) as pbar:\n",
        "        for _ in range(n_iter):\n",
        "            # Sample hyperparameters\n",
        "            hyperparams = {\n",
        "                'embedding_dim': np.random.uniform(*param_space['embedding_dim']),\n",
        "                'hidden_dim': np.random.uniform(*param_space['hidden_dim']),\n",
        "                'lr': 10**np.random.uniform(*param_space['lr']),\n",
        "                'epochs': np.random.randint(*param_space['epochs']),\n",
        "            }\n",
        "            f1, accuracy, model = train_and_evaluate(hyperparams, train_loader, val_loader, vocab_size, output_dim, device)\n",
        "            pbar.set_postfix(f1=f1, accuracy=accuracy)\n",
        "            pbar.update(1)\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_params = hyperparams\n",
        "                best_model = model\n",
        "\n",
        "    print(f\"Best Validation F1 Score: {best_f1:.4f}\")\n",
        "    print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "    # Save the best model\n",
        "    torch.save(best_model.state_dict(), \"best_gru_model.pth\")\n",
        "    return best_params, best_f1, best_model\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "# Run random search\n",
        "best_params, best_f1, best_model= random_hyperparameter_tuning(\n",
        "    train_loader_3,\n",
        "    val_loader_3,\n",
        "    vocab_size=None,\n",
        "    output_dim=2,\n",
        "    device=device,\n",
        "    n_iter=10  # Number of random samples to evaluate\n",
        ")\n",
        "# End timer\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f\"Total Parameter Tuning Time: {total_time:.2f} seconds\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpOl8zid6le4",
        "ExecuteTime": {
          "end_time": "2025-10-17T11:41:39.389512Z",
          "start_time": "2025-10-17T11:31:56.334786Z"
        }
      },
      "source": [
        "\n",
        "\n",
        "# ===========Fixed hyperparameters (random search results)==============\n",
        "hyperparams = {\n",
        "    'embedding_dim': 186.88240060019746,\n",
        "    'hidden_dim': 733.7677322150511,\n",
        "    'lr': 0.000448103301026688,\n",
        "    'epochs': 8\n",
        "}\n",
        "\n",
        "train_loader = train_loader_3\n",
        "val_loader=val_loader_3\n",
        "\n",
        "output_dim=2\n",
        "device=device\n",
        "embedding_dim = int(hyperparams['embedding_dim'])\n",
        "hidden_dim = int(hyperparams['hidden_dim'])\n",
        "lr = float(hyperparams['lr'])\n",
        "\n",
        "\n",
        "# Initialize the model, loss, and optimizer\n",
        "model = GRUSentimentModel(\n",
        "        num_cols=num_cols,\n",
        "        cat_cols=cat_cols,\n",
        "        cat_num_embeddings=cat_num_embeddings,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        output_dim=output_dim\n",
        "    ).to(device)\n",
        "\n",
        "# =====Class weights (addressing severe imbalance)=====\n",
        "all_train_labels = []\n",
        "for batch in train_loader:\n",
        "    all_train_labels.extend(batch['label'].numpy())\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(all_train_labels),\n",
        "    y=all_train_labels\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)  # Convert to PyTorch tensor\n",
        "\n",
        "# =====Define loss function and optimizer=====\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# =====Training + validation loop=====\n",
        "for epoch in range(hyperparams['epochs']):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    train_loop = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{hyperparams['epochs']}\", leave=True, position=0)\n",
        "    for batch in train_loop:\n",
        "        num_tensor, cat_tensor = batch['input_ids']\n",
        "        num_tensor = num_tensor.to(device).float()\n",
        "        cat_tensor = cat_tensor.to(device).long()\n",
        "        labels = batch['label'].to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model((num_tensor, cat_tensor))\n",
        "        loss = criterion(outputs, labels)  # **MODIFIED**: Use CrossEntropyLoss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{hyperparams['epochs']}, Loss: {total_loss / len(train_loader):.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziuemfDo9UMG",
        "ExecuteTime": {
          "end_time": "2025-10-17T11:42:04.538915Z",
          "start_time": "2025-10-17T11:42:04.518533Z"
        }
      },
      "source": [
        "model_save_path = \"GRU_Binary.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-17T11:45:29.480168Z",
          "start_time": "2025-10-17T11:45:00.771743Z"
        },
        "id": "gwf6cjxGnare"
      },
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# =========================================================\n",
        "# GRU: Permutation Importance (by original columns: num_cols + cat_cols)\n",
        "# =========================================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Helper function: perform inference on DataFrame and return y_pred\n",
        "@torch.no_grad()\n",
        "def _gru_predict_on_df(model, X_df, y_ser, num_cols, cat_cols, num_pipeline, cat_to_index,\n",
        "                       batch_size=512, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    ds = TabularGRUDataset(X_df, y_ser, num_cols, cat_cols, num_pipeline, cat_to_index)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "    preds = []\n",
        "    for batch in dl:\n",
        "        num_t, cat_t = batch['input_ids']\n",
        "        num_t = num_t.to(device).float()\n",
        "        cat_t = cat_t.to(device).long()\n",
        "        logits = model((num_t, cat_t))\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        preds.extend(pred.cpu().numpy().tolist())\n",
        "    return np.array(preds, dtype=np.int64)\n",
        "\n",
        "# Main function: compute GRU permutation importance and generate plots/exports\n",
        "def run_gru_permutation_importance(model,      # Trained GRU model (already to(device))\n",
        "                                   X_df, y_ser,# Evaluation data (recommend using validation set X_val_df / y_val_df)\n",
        "                                   num_cols, cat_cols,\n",
        "                                   num_pipeline, cat_to_index,\n",
        "                                   model_tag=\"GRU\",\n",
        "                                   n_repeats=6,\n",
        "                                   max_subsample=3000,     # Subsample upper limit for speed\n",
        "                                   batch_size=512,\n",
        "                                   device=None,\n",
        "                                   out_dir=\"./importance_outputs\",\n",
        "                                   top_n=20):\n",
        "    \"\"\"\n",
        "    Perform permutation for each column (numeric / categorical):\n",
        "      importance = baseline_f1 - mean(f1_after_permute)\n",
        "    Feature names use original column names (num_cols + cat_cols)\n",
        "    \"\"\"\n",
        "    assert device is not None, \"please pass device (cpu/cuda)\"\n",
        "\n",
        "    # ---- Subsample for speed (consistent with fast version for SVM above) ----\n",
        "    n_full = len(X_df)\n",
        "    take = min(max_subsample, n_full)\n",
        "    if take < n_full:\n",
        "        # Stratified sampling (maintain y distribution)\n",
        "        from sklearn.model_selection import StratifiedShuffleSplit\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, train_size=take, random_state=42)\n",
        "        idx = next(sss.split(np.zeros(n_full), y_ser))[0]\n",
        "        X_use = X_df.iloc[idx].reset_index(drop=True).copy()\n",
        "        y_use = y_ser.iloc[idx].reset_index(drop=True).copy()\n",
        "        print(f\"[{model_tag}] Using subsample: {take}/{n_full}\")\n",
        "    else:\n",
        "        X_use = X_df.reset_index(drop=True).copy()\n",
        "        y_use = y_ser.reset_index(drop=True).copy()\n",
        "\n",
        "    # ---- baseline ----\n",
        "    base_pred = _gru_predict_on_df(model, X_use, y_use, num_cols, cat_cols, num_pipeline, cat_to_index,\n",
        "                                   batch_size=batch_size, device=device)\n",
        "    base_f1 = f1_score(y_use.values, base_pred, average='weighted')\n",
        "    print(f\"[{model_tag}] Baseline F1 (weighted): {base_f1:.4f}\")\n",
        "\n",
        "    # ---- Perform permutation for each feature ----\n",
        "    feat_names = list(num_cols) + list(cat_cols)\n",
        "    importances = []\n",
        "    for col in feat_names:\n",
        "        scores = []\n",
        "        for _ in range(n_repeats):\n",
        "            X_perm = X_use.copy()\n",
        "            # Shuffle rows within this column (preserve marginal distribution)\n",
        "            X_perm[col] = np.random.permutation(X_perm[col].values)\n",
        "            pred_p = _gru_predict_on_df(model, X_perm, y_use, num_cols, cat_cols, num_pipeline, cat_to_index,\n",
        "                                        batch_size=batch_size, device=device)\n",
        "            f1_p = f1_score(y_use.values, pred_p, average='weighted')\n",
        "            scores.append(f1_p)\n",
        "        imp = max(0.0, base_f1 - float(np.mean(scores)))  # F1 decrease is treated as importance (non-negative)\n",
        "        importances.append(imp)\n",
        "\n",
        "    importance_df = pd.DataFrame({'feature': feat_names, 'importance': importances}) \\\n",
        "                     .sort_values('importance', ascending=False) \\\n",
        "                     .reset_index(drop=True)\n",
        "\n",
        "    # ---- Export CSV ----\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    csv_path = os.path.join(out_dir, f\"{model_tag}_perm_importances_mean.csv\")\n",
        "    importance_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"[{model_tag}] Saved permutation importance CSV -> {csv_path}\")\n",
        "\n",
        "    # ---- Main plot: Top 20 horizontal bar ----\n",
        "    top_df = importance_df.head(top_n).sort_values('importance', ascending=True)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    ax = sns.barplot(x='importance', y='feature', data=top_df, orient='h')\n",
        "    plt.title(f\"{model_tag} - Top {top_n} Feature Importance\")\n",
        "    plt.xlabel(\"Importance (Î”F1)\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    for i, (val, feat) in enumerate(zip(top_df['importance'], top_df['feature'])):\n",
        "        ax.text(val + 1e-6, i, f\"{val:.4f}\", va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Supplementary plot: Cumulative importance curve ----\n",
        "    vals = importance_df['importance'].values\n",
        "    total = vals.sum() if vals.sum() != 0 else 1.0\n",
        "    cum = np.cumsum(vals) / total * 100\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(range(1, len(cum)+1), cum, marker='o')\n",
        "    plt.xlabel(\"Number of features included\")\n",
        "    plt.ylabel(\"Cumulative importance (%)\")\n",
        "    plt.title(f\"{model_tag} cumulative importance (%)\")\n",
        "    plt.grid(True)\n",
        "    plt.axhline(80, color='gray', linestyle='--', linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "\n",
        "gru_imp_df = run_gru_permutation_importance(\n",
        "    model,\n",
        "    X_train_df, y_train_df,      # If X_val_df/y_val_df not available\n",
        "    num_cols, cat_cols,\n",
        "    num_pipeline, cat_to_index,\n",
        "    model_tag=\"GRU\",\n",
        "    n_repeats=6,             # Start with 6 for quick run; can use 10~20 for final publication\n",
        "    max_subsample=3000,      # Subsample for speed; can use 5000~10000 on better machines\n",
        "    batch_size=512,\n",
        "    device=device,           # Keep consistent with training\n",
        "    out_dir=\"./importance_outputs\",\n",
        "    top_n=20\n",
        ")\n",
        "\n",
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-08T14:35:29.921936Z",
          "start_time": "2025-09-08T14:35:19.069606Z"
        },
        "id": "pNsnsIJFrzKW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "# Evaluate the best model on the test set\n",
        "best_model = model\n",
        "best_model.eval()\n",
        "test_predictions, test_labels = [], []\n",
        "\n",
        "# **MODIFIED**: Added probabilities collection for further analysis if needed\n",
        "test_probabilities = []\n",
        "test_loop = tqdm(test_loader_3, desc=\"Testing Best Model\", leave=True, position=0)\n",
        "with torch.no_grad():\n",
        "    for batch in test_loop:\n",
        "        num_tensor, cat_tensor = batch['input_ids']\n",
        "        num_tensor = num_tensor.to(device).float()\n",
        "        cat_tensor = cat_tensor.to(device).long()\n",
        "        labels = batch['label'].to(device).long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = best_model((num_tensor, cat_tensor))\n",
        "\n",
        "        # **MODIFIED**: Added probability calculation using softmax\n",
        "        probs = torch.softmax(outputs, dim=1)  # Probabilities for each class\n",
        "        preds = torch.argmax(probs, dim=1)  # Predicted class\n",
        "\n",
        "        test_predictions.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # **MODIFIED**: Collect probabilities of the positive class (e.g., class 1)\n",
        "        test_probabilities.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "# Calculate F1 score and accuracy on the test set\n",
        "test_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Test F-1 Score: {test_f1:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, test_predictions, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-09T06:36:32.232908Z",
          "start_time": "2025-09-09T06:35:57.637198Z"
        },
        "id": "CiQrZUzDE9-H"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_pytorch_model(model, test_loader, device, model_name=\"GRU\", num_classes=2):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Keep consistent with training: numeric features + categorical features\n",
        "            num_tensor, cat_tensor = batch['input_ids']\n",
        "            num_tensor = num_tensor.to(device).float()\n",
        "            cat_tensor = cat_tensor.to(device).long()\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model((num_tensor, cat_tensor))  # Get logits directly\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        positive_probs = all_probs[:, 1]\n",
        "    else:\n",
        "        positive_probs = None\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    print(f\"\\n--- Evaluation Report: {model_name} ---\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "    # F1 CI\n",
        "    f1_scores = []\n",
        "    for _ in range(500):  # bootstrap\n",
        "        idx = resample(np.arange(len(all_labels)))\n",
        "        if len(np.unique(all_labels[idx])) < 2:\n",
        "            continue\n",
        "        f1_bs = f1_score(all_labels[idx], all_preds[idx], average='weighted')\n",
        "        f1_scores.append(f1_bs)\n",
        "    if f1_scores:\n",
        "        print(f\"95% CI for F1 Score: [{np.percentile(f1_scores, 2.5):.4f}, {np.percentile(f1_scores, 97.5):.4f}]\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    # ROC\n",
        "    if num_classes == 2:\n",
        "        fpr, tpr, _ = roc_curve(all_labels, positive_probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
        "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(f\"ROC Curve - {model_name}\")\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show()\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "\n",
        "gru = GRUSentimentModel(\n",
        "    num_cols=num_cols,\n",
        "    cat_cols=cat_cols,\n",
        "    cat_num_embeddings=cat_num_embeddings,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "gru.load_state_dict(torch.load(\"GRU_Binary.pth\", map_location=device))\n",
        "gru.to(device)\n",
        "evaluate_pytorch_model(gru, test_loader_3, device, model_name=\"gru\", num_classes=2)\n",
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}